{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# AVF Acoustic Analysis: Machine Learning Classification\n",
        "\n",
        "This notebook performs:\n",
        "- Phase 1: Comparison of engineered vs. perceptual acoustic features\n",
        "- Phase 2: Comparison of acoustic-only vs. acoustic + clinical features\n",
        "\n",
        "Methods:\n",
        "- Patient-level data splitting\n",
        "- Stability selection with bootstrap (500 iterations)\n",
        "- Logistic Regression and Random Forest classifiers\n",
        "- SHAP analysis for model interpretability"
      ],
      "metadata": {
        "id": "header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    !pip install python-docx statsmodels shap openpyxl -q\n",
        "    !apt-get -qq install -y fonts-liberation\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "from sklearn.model_selection import train_test_split, GroupKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import (\n",
        "    classification_report, roc_auc_score, RocCurveDisplay,\n",
        "    brier_score_loss\n",
        ")\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.base import clone\n",
        "from sklearn.calibration import CalibrationDisplay\n",
        "from sklearn.compose import make_column_selector, ColumnTransformer\n",
        "\n",
        "from scipy.stats import mannwhitneyu, chi2_contingency, norm\n",
        "import matplotlib.pyplot as plt\n",
        "from docx import Document\n",
        "from statsmodels.stats.multitest import multipletests\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "plt.rcParams['font.family'] = 'Liberation Sans'"
      ],
      "metadata": {
        "id": "imports"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TARGET_COLUMN_FOR_LABELING = 'FV'  # 'FV' or 'RI'\n",
        "GROUP_THRESHOLD = 400              # FV: 400, RI: 0.6\n",
        "\n",
        "DRIVE_BASE_PATH = Path(\"/path/to/your/data/folder/\")\n",
        "INPUT_EXCEL_PATH = DRIVE_BASE_PATH / \"your_data.xlsx\"\n",
        "OUTPUT_DIR = DRIVE_BASE_PATH / f\"{TARGET_COLUMN_FOR_LABELING}_{GROUP_THRESHOLD}_results\"\n",
        "\n",
        "N_FEATURES_TO_SELECT = 5\n",
        "N_SPLITS_CV = 5\n",
        "N_BOOTSTRAPS_STABILITY = 500\n",
        "N_BOOTSTRAPS_CI = 1000\n",
        "UNIFIED_RANDOM_STATE = 243"
      ],
      "metadata": {
        "id": "config"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_preprocess_data(file_path: Path, target_column: str, threshold: float):\n",
        "    if not file_path.exists():\n",
        "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
        "\n",
        "    df = pd.read_excel(file_path)\n",
        "    target_variable = f'{target_column}>={threshold}'\n",
        "    df[target_variable] = (df[target_column] >= threshold).astype(int)\n",
        "\n",
        "    print(f\"Loaded data from '{file_path.name}'\")\n",
        "    print(f\"Created binary target '{target_variable}' from '{target_column}'\")\n",
        "    print(f\"n={len(df)} samples\")\n",
        "\n",
        "    return target_variable, df\n",
        "\n",
        "\n",
        "def split_data_by_patient(df: pd.DataFrame, target_variable: str,\n",
        "                          test_size: float = 0.2, random_state: int = None):\n",
        "    unique_patients = df['Pt No'].unique()\n",
        "    patient_labels = df.drop_duplicates(subset=['Pt No']).set_index('Pt No')[target_variable]\n",
        "\n",
        "    train_patients, test_patients = train_test_split(\n",
        "        unique_patients, test_size=test_size, random_state=random_state,\n",
        "        stratify=patient_labels.reindex(unique_patients)\n",
        "    )\n",
        "\n",
        "    train_indices = df[df['Pt No'].isin(train_patients)].index\n",
        "    test_indices = df[df['Pt No'].isin(test_patients)].index\n",
        "\n",
        "    print(f\"\\nPatient-level split: Train={len(train_patients)}, Test={len(test_patients)}\")\n",
        "    print(f\"Sample-level split: Train={len(train_indices)}, Test={len(test_indices)}\")\n",
        "\n",
        "    return train_indices, test_indices"
      ],
      "metadata": {
        "id": "data_loading"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_patient_characteristics_table(df: pd.DataFrame, target_variable: str) -> pd.DataFrame:\n",
        "    print(\"\\n--- Creating Table 1: Patient Characteristics ---\")\n",
        "    df_patients = df.drop_duplicates(subset=['Pt No']).copy()\n",
        "\n",
        "    group0 = df_patients[df_patients[target_variable] == 0]\n",
        "    group1 = df_patients[df_patients[target_variable] == 1]\n",
        "\n",
        "    table_data = []\n",
        "\n",
        "    continuous_vars = ['age', 'POD', 'FV', 'RI', 'RA diameter', 'shunt diameter']\n",
        "    for var in continuous_vars:\n",
        "        if var not in df.columns:\n",
        "            continue\n",
        "\n",
        "        n_missing0 = group0[var].isnull().sum()\n",
        "        n_missing1 = group1[var].isnull().sum()\n",
        "\n",
        "        mean0, std0 = group0[var].mean(), group0[var].std()\n",
        "        mean1, std1 = group1[var].mean(), group1[var].std()\n",
        "\n",
        "        mean0_str = f\"{mean0:.1f} ± {std0:.1f}\"\n",
        "        if n_missing0 > 0:\n",
        "            mean0_str += f\" (missing: {n_missing0})\"\n",
        "\n",
        "        mean1_str = f\"{mean1:.1f} ± {std1:.1f}\"\n",
        "        if n_missing1 > 0:\n",
        "            mean1_str += f\" (missing: {n_missing1})\"\n",
        "\n",
        "        _, p_val = mannwhitneyu(group0[var].dropna(), group1[var].dropna())\n",
        "\n",
        "        table_data.append({\n",
        "            'Characteristic': var,\n",
        "            f'{TARGET_COLUMN_FOR_LABELING} < {GROUP_THRESHOLD} (n={len(group0)})': mean0_str,\n",
        "            f'{TARGET_COLUMN_FOR_LABELING} >= {GROUP_THRESHOLD} (n={len(group1)})': mean1_str,\n",
        "            'P-Value': f\"{p_val:.2f}\" if p_val >= 0.01 else \"<0.01\"\n",
        "        })\n",
        "\n",
        "    categorical_vars = {\n",
        "        'men': 'Male Sex (n, %)',\n",
        "        'arterial calcification': 'Arterial Calcification (n, %)',\n",
        "        'DM': 'DM (n, %)',\n",
        "        'HTN': 'HTN (n, %)',\n",
        "        'Heart disease': 'Heart Disease (n, %)',\n",
        "        'tabaco site': 'tabaco site (n, %)',\n",
        "        'left': 'left (n, %)'\n",
        "    }\n",
        "\n",
        "    mapping_dict = {'y': 1, 'n': 0, 'male': 1, 'female': 0, '1': 1, '0': 0, '1.0': 1, '0.0': 0}\n",
        "\n",
        "    for var, name in categorical_vars.items():\n",
        "        if var not in df_patients.columns:\n",
        "            continue\n",
        "\n",
        "        df_patients[var + '_numeric'] = df_patients[var].astype(str).str.lower().map(mapping_dict)\n",
        "        df_filtered = df_patients.dropna(subset=[var + '_numeric'])\n",
        "\n",
        "        group0_cat = df_filtered[df_filtered[target_variable] == 0]\n",
        "        group1_cat = df_filtered[df_filtered[target_variable] == 1]\n",
        "\n",
        "        crosstab = pd.crosstab(df_filtered[var + '_numeric'], df_filtered[target_variable])\n",
        "\n",
        "        p_val = chi2_contingency(crosstab)[1] if crosstab.shape == (2, 2) else 1.0\n",
        "\n",
        "        n0_pos = crosstab.loc[1, 0] if 1 in crosstab.index and 0 in crosstab.columns else 0\n",
        "        n1_pos = crosstab.loc[1, 1] if 1 in crosstab.index and 1 in crosstab.columns else 0\n",
        "\n",
        "        percent0 = (n0_pos / len(group0_cat) * 100) if len(group0_cat) > 0 else 0\n",
        "        percent1 = (n1_pos / len(group1_cat) * 100) if len(group1_cat) > 0 else 0\n",
        "\n",
        "        table_data.append({\n",
        "            'Characteristic': name,\n",
        "            f'{TARGET_COLUMN_FOR_LABELING} < {GROUP_THRESHOLD} (n={len(group0)})': f\"{n0_pos} ({percent0:.1f})\",\n",
        "            f'{TARGET_COLUMN_FOR_LABELING} >= {GROUP_THRESHOLD} (n={len(group1)})': f\"{n1_pos} ({percent1:.1f})\",\n",
        "            'P-Value': f\"{p_val:.2f}\" if p_val >= 0.01 else \"<0.01\"\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(table_data)"
      ],
      "metadata": {
        "id": "table_creation"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def select_stable_features(X_train_df: pd.DataFrame, y_train: pd.Series,\n",
        "                          groups_train: pd.Series, feature_list: list,\n",
        "                          model_for_select, n_features_to_select: int,\n",
        "                          n_bootstraps: int, random_state: int) -> list:\n",
        "    print(f\"-> Starting stability selection (bootstrap iterations: {n_bootstraps})...\")\n",
        "\n",
        "    feature_counter = Counter()\n",
        "    unique_patients = groups_train.unique()\n",
        "    n_patients = len(unique_patients)\n",
        "    rng = np.random.RandomState(random_state)\n",
        "\n",
        "    if isinstance(model_for_select, Pipeline):\n",
        "        scaler = clone(model_for_select.steps[0][1])\n",
        "        estimator = clone(model_for_select.steps[-1][1])\n",
        "    else:\n",
        "        scaler = None\n",
        "        estimator = clone(model_for_select)\n",
        "\n",
        "    selector = RFE(estimator=estimator, n_features_to_select=n_features_to_select, step=1)\n",
        "    X_train_subset = X_train_df[feature_list]\n",
        "\n",
        "    valid_iterations = 0\n",
        "    for i in range(n_bootstraps):\n",
        "        if (i + 1) % 100 == 0:\n",
        "            print(f\"  ... Bootstrap {i+1}/{n_bootstraps} complete\")\n",
        "\n",
        "        try:\n",
        "            sampled_patients = rng.choice(unique_patients, size=n_patients, replace=True)\n",
        "            mask = groups_train.isin(sampled_patients)\n",
        "            idx = groups_train[mask].index\n",
        "\n",
        "            if len(idx) == 0 or len(np.unique(y_train.loc[idx])) < 2:\n",
        "                continue\n",
        "\n",
        "            X_boot = X_train_subset.loc[idx]\n",
        "            y_boot = y_train.loc[idx]\n",
        "\n",
        "            X_boot_scaled = scaler.fit_transform(X_boot) if scaler else X_boot\n",
        "\n",
        "            selector.fit(X_boot_scaled, y_boot)\n",
        "            selected = X_boot.columns[selector.support_].tolist()\n",
        "\n",
        "            feature_counter.update(selected)\n",
        "            valid_iterations += 1\n",
        "\n",
        "        except Exception:\n",
        "            continue\n",
        "\n",
        "    if valid_iterations == 0:\n",
        "        print(\"Warning: No valid bootstrap iterations.\")\n",
        "        return feature_list[:n_features_to_select]\n",
        "\n",
        "    most_common = [f for f, _ in feature_counter.most_common(n_features_to_select)]\n",
        "\n",
        "    print(f\"-> Stability selection complete. ({valid_iterations}/{n_bootstraps} valid iterations)\")\n",
        "    print(\"--- Selected features (by frequency) ---\")\n",
        "    for feature, count in feature_counter.most_common(10):\n",
        "        print(f\"  {feature}: {count} ({count/valid_iterations:.1%})\")\n",
        "\n",
        "    return most_common"
      ],
      "metadata": {
        "id": "feature_selection"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model_with_cv(X_selected: pd.DataFrame, y: pd.Series,\n",
        "                          groups: pd.Series, model) -> pd.DataFrame:\n",
        "    group_kfold = GroupKFold(n_splits=N_SPLITS_CV)\n",
        "    scores = {\n",
        "        'auc': [], 'accuracy': [], 'sensitivity': [],\n",
        "        'specificity': [], 'f1_score': []\n",
        "    }\n",
        "\n",
        "    for train_idx, test_idx in group_kfold.split(X_selected, y, groups):\n",
        "        X_train_cv = X_selected.iloc[train_idx]\n",
        "        X_test_cv = X_selected.iloc[test_idx]\n",
        "        y_train_cv = y.iloc[train_idx]\n",
        "        y_test_cv = y.iloc[test_idx]\n",
        "\n",
        "        model_clone = clone(model)\n",
        "        model_clone.fit(X_train_cv, y_train_cv)\n",
        "\n",
        "        y_pred_proba = model_clone.predict_proba(X_test_cv)[:, 1]\n",
        "        y_pred = model_clone.predict(X_test_cv)\n",
        "\n",
        "        if len(np.unique(y_test_cv)) > 1:\n",
        "            scores['auc'].append(roc_auc_score(y_test_cv, y_pred_proba))\n",
        "        else:\n",
        "            scores['auc'].append(np.nan)\n",
        "\n",
        "        report = classification_report(y_test_cv, y_pred, output_dict=True, zero_division=0)\n",
        "        scores['accuracy'].append(report['accuracy'])\n",
        "        scores['sensitivity'].append(report.get('1', {}).get('recall', np.nan))\n",
        "        scores['specificity'].append(report.get('0', {}).get('recall', np.nan))\n",
        "        scores['f1_score'].append(report['weighted avg']['f1-score'])\n",
        "\n",
        "    return pd.DataFrame(scores).agg(['mean', 'std']).T\n",
        "\n",
        "\n",
        "def calculate_bootstrap_ci(y_test: pd.Series, y_pred_proba: np.ndarray,\n",
        "                          test_patient_ids: pd.Series) -> tuple:\n",
        "    unique_patients = test_patient_ids.unique()\n",
        "    n_patients = len(unique_patients)\n",
        "    rng = np.random.RandomState(UNIFIED_RANDOM_STATE)\n",
        "\n",
        "    bootstrapped_scores = []\n",
        "    for _ in range(N_BOOTSTRAPS_CI):\n",
        "        sampled_patients = rng.choice(unique_patients, size=n_patients, replace=True)\n",
        "        mask = test_patient_ids.isin(sampled_patients)\n",
        "        idx = test_patient_ids[mask].index\n",
        "\n",
        "        if len(idx) == 0:\n",
        "            continue\n",
        "\n",
        "        y_true_bs = y_test.loc[idx]\n",
        "        idx_positions = y_test.index.get_indexer(idx)\n",
        "        y_pred_bs = y_pred_proba[idx_positions]\n",
        "\n",
        "        if len(np.unique(y_true_bs)) < 2:\n",
        "            continue\n",
        "\n",
        "        bootstrapped_scores.append(roc_auc_score(y_true_bs, y_pred_bs))\n",
        "\n",
        "    if not bootstrapped_scores:\n",
        "        return np.nan, np.nan\n",
        "\n",
        "    return np.percentile(bootstrapped_scores, 2.5), np.percentile(bootstrapped_scores, 97.5)\n",
        "\n",
        "\n",
        "def get_feature_importances(model, columns: list) -> pd.Series:\n",
        "    estimator = model.steps[-1][1] if isinstance(model, Pipeline) else model\n",
        "\n",
        "    if hasattr(estimator, 'feature_importances_'):\n",
        "        importances = estimator.feature_importances_\n",
        "    elif hasattr(estimator, 'coef_'):\n",
        "        importances = np.abs(estimator.coef_[0])\n",
        "    else:\n",
        "        importances = np.zeros(len(columns))\n",
        "\n",
        "    return pd.Series(importances, index=columns).sort_values(ascending=False)"
      ],
      "metadata": {
        "id": "model_evaluation"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_midrank(x):\n",
        "    J = np.argsort(x)\n",
        "    Z = x[J]\n",
        "    N = len(x)\n",
        "    T = np.zeros(N, dtype=float)\n",
        "    i = 0\n",
        "    while i < N:\n",
        "        j = i\n",
        "        while j < N and Z[j] == Z[i]:\n",
        "            j += 1\n",
        "        T[i:j] = 0.5 * (i + j - 1)\n",
        "        i = j\n",
        "    T2 = np.empty(N, dtype=float)\n",
        "    T2[J] = T + 1\n",
        "    return T2\n",
        "\n",
        "\n",
        "def fast_delong_auc_cov(y_true, y_pred):\n",
        "    y_true = np.asarray(y_true)\n",
        "    y_pred = np.asarray(y_pred)\n",
        "\n",
        "    m = sum(y_true == 1)\n",
        "    n = sum(y_true == 0)\n",
        "\n",
        "    v_10 = compute_midrank(y_pred[y_true == 1])\n",
        "    v_01 = compute_midrank(y_pred[y_true == 0])\n",
        "\n",
        "    auc = (np.sum(v_10) - m * (m + 1) / 2) / (m * n)\n",
        "\n",
        "    s_10 = v_10 / m - (m + 1) / (2 * m)\n",
        "    s_01 = v_01 / n - (n + 1) / (2 * n)\n",
        "\n",
        "    var_10 = (np.sum(s_10**2) - m * (s_10.sum()/m)**2) / (m - 1)\n",
        "    var_01 = (np.sum(s_01**2) - n * (s_01.sum()/n)**2) / (n - 1)\n",
        "\n",
        "    return auc, var_10, var_01\n",
        "\n",
        "\n",
        "def delong_roc_test(y_true, scores_a, scores_b):\n",
        "    y_true = np.asarray(y_true)\n",
        "    scores_a = np.asarray(scores_a)\n",
        "    scores_b = np.asarray(scores_b)\n",
        "\n",
        "    m = sum(y_true == 1)\n",
        "    n = sum(y_true == 0)\n",
        "\n",
        "    if m == 0 or n == 0:\n",
        "        return 1.0\n",
        "\n",
        "    auc_a, var_a_10, var_a_01 = fast_delong_auc_cov(y_true, scores_a)\n",
        "    auc_b, var_b_10, var_b_01 = fast_delong_auc_cov(y_true, scores_b)\n",
        "\n",
        "    v_a_10 = compute_midrank(scores_a[y_true == 1])\n",
        "    v_a_01 = compute_midrank(scores_a[y_true == 0])\n",
        "    v_b_10 = compute_midrank(scores_b[y_true == 1])\n",
        "    v_b_01 = compute_midrank(scores_b[y_true == 0])\n",
        "\n",
        "    s_a_10 = v_a_10 / m - (m + 1) / (2 * m)\n",
        "    s_a_01 = v_a_01 / n - (n + 1) / (2 * n)\n",
        "    s_b_10 = v_b_10 / m - (m + 1) / (2 * m)\n",
        "    s_b_01 = v_b_01 / n - (n + 1) / (2 * n)\n",
        "\n",
        "    cov_10 = (np.sum(s_a_10 * s_b_10) - m * (s_a_10.sum()/m) * (s_b_10.sum()/m)) / (m - 1)\n",
        "    cov_01 = (np.sum(s_a_01 * s_b_01) - n * (s_a_01.sum()/n) * (s_b_01.sum()/n)) / (n - 1)\n",
        "\n",
        "    var_diff = var_a_10 / m + var_a_01 / n + var_b_10 / m + var_b_01 / n - 2 * (cov_10 / m + cov_01 / n)\n",
        "\n",
        "    if var_diff <= 1e-8:\n",
        "        return 1.0\n",
        "\n",
        "    z = (auc_a - auc_b) / np.sqrt(var_diff)\n",
        "    p = 2 * norm.sf(np.abs(z))\n",
        "\n",
        "    return p\n",
        "\n",
        "\n",
        "def compute_auc_diff_paired(y_test: pd.Series, y_pred_probas: pd.DataFrame,\n",
        "                           test_patient_ids: pd.Series, model_pair: tuple) -> dict:\n",
        "    model_a, model_b = model_pair\n",
        "    proba_a = y_pred_probas[model_a]\n",
        "    proba_b = y_pred_probas[model_b]\n",
        "\n",
        "    unique_patients = test_patient_ids.unique()\n",
        "    n_patients = len(unique_patients)\n",
        "    rng = np.random.RandomState(UNIFIED_RANDOM_STATE)\n",
        "\n",
        "    auc_diffs = []\n",
        "    for _ in range(N_BOOTSTRAPS_CI):\n",
        "        sampled_patients = rng.choice(unique_patients, size=n_patients, replace=True)\n",
        "        mask = test_patient_ids.isin(sampled_patients)\n",
        "        idx = test_patient_ids[mask].index\n",
        "\n",
        "        if len(idx) == 0:\n",
        "            continue\n",
        "\n",
        "        y_true_bs = y_test.loc[idx]\n",
        "\n",
        "        if len(np.unique(y_true_bs)) < 2:\n",
        "            continue\n",
        "\n",
        "        auc_a = roc_auc_score(y_true_bs, proba_a.loc[idx])\n",
        "        auc_b = roc_auc_score(y_true_bs, proba_b.loc[idx])\n",
        "        auc_diffs.append(auc_a - auc_b)\n",
        "\n",
        "    if not auc_diffs:\n",
        "        return {\n",
        "            'model_pair': f\"{model_a} vs {model_b}\",\n",
        "            'auc_diff_mean': np.nan,\n",
        "            'ci_lower': np.nan,\n",
        "            'ci_upper': np.nan,\n",
        "            'p_value': np.nan\n",
        "        }\n",
        "\n",
        "    auc_diffs = np.array(auc_diffs)\n",
        "    mean_diff = auc_diffs.mean()\n",
        "    ci_lower = np.percentile(auc_diffs, 2.5)\n",
        "    ci_upper = np.percentile(auc_diffs, 97.5)\n",
        "\n",
        "    p_val = min(\n",
        "        np.mean(auc_diffs <= 0),\n",
        "        np.mean(auc_diffs >= 0)\n",
        "    ) * 2\n",
        "    p_val = min(p_val, 1.0)\n",
        "\n",
        "    return {\n",
        "        'model_pair': f\"{model_a} vs {model_b}\",\n",
        "        'auc_diff_mean': mean_diff,\n",
        "        'ci_lower': ci_lower,\n",
        "        'ci_upper': ci_upper,\n",
        "        'p_value': p_val\n",
        "    }"
      ],
      "metadata": {
        "id": "auc_comparison"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_and_plot_shap(model, X_train, X_test, output_dir, model_name, target_name):\n",
        "    import shap\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"--- SHAP Analysis ---\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    plt.rcParams['font.family'] = 'Liberation Sans'\n",
        "\n",
        "    model_to_explain = model.steps[-1][1] if isinstance(model, Pipeline) else model\n",
        "    print(f\"Initializing SHAP explainer for: {type(model_to_explain).__name__}\")\n",
        "\n",
        "    explainer = shap.Explainer(model_to_explain, X_train)\n",
        "    explanation = explainer(X_test)\n",
        "\n",
        "    explanation_plot = explanation[:, :, 1] if hasattr(explanation, 'values') and explanation.values.ndim == 3 else explanation\n",
        "\n",
        "    print(\"Generating SHAP Summary Plot...\")\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    shap.summary_plot(explanation_plot, features=X_test, show=False, plot_size=None)\n",
        "    fig_summary = output_dir / f'Figure2_SHAP_Summary_{target_name}.svg'\n",
        "    plt.title(f'Figure 2: SHAP Summary Plot ({model_name})', fontsize=16, weight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(fig_summary, format='svg', bbox_inches='tight')\n",
        "    plt.show()\n",
        "    print(f\"Saved to '{fig_summary}'\")\n",
        "\n",
        "    return [fig_summary]\n",
        "\n",
        "\n",
        "def plot_calibration_curve(model, X_test, y_test, model_name, output_dir, target_name):\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"--- Calibration Analysis ---\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
        "    brier = brier_score_loss(y_test, y_pred_proba)\n",
        "\n",
        "    print(f\"Brier Score for '{model_name}': {brier:.4f}\")\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(10, 8))\n",
        "    CalibrationDisplay.from_predictions(\n",
        "        y_test, y_pred_proba, n_bins=5, name=model_name,\n",
        "        ax=ax, strategy='uniform'\n",
        "    )\n",
        "\n",
        "    ax.set_title(f'Figure 4: Calibration Plot ({model_name})', fontsize=16, weight='bold')\n",
        "    ax.set_xlabel(\"Mean Predicted Probability\", fontsize=12, weight='bold')\n",
        "    ax.set_ylabel(\"Fraction of Positives\", fontsize=12, weight='bold')\n",
        "    ax.grid(linestyle=':', alpha=0.6)\n",
        "\n",
        "    fig_path = output_dir / f'Figure4_Calibration_{target_name}.svg'\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(fig_path, format='svg', bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"Saved to '{fig_path}'\")\n",
        "    return fig_path\n",
        "\n",
        "\n",
        "def plot_roc_curves(final_results: dict, table2_df: pd.DataFrame, output_file: Path):\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    ax = plt.gca()\n",
        "\n",
        "    for model_id in table2_df['Model'].unique():\n",
        "        if model_id not in final_results:\n",
        "            continue\n",
        "\n",
        "        result = final_results[model_id]\n",
        "        cv_auc = table2_df.loc[table2_df['Model'] == model_id, 'AUC'].values[0].split(' ±')[0]\n",
        "        label = f\"{model_id} (AUC = {float(cv_auc):.3f})\"\n",
        "\n",
        "        RocCurveDisplay.from_estimator(\n",
        "            result['model'], result['X_test'], result['y_test'],\n",
        "            name=label, ax=ax\n",
        "        )\n",
        "\n",
        "    plt.plot([0, 1], [0, 1], color='black', lw=1, linestyle='--')\n",
        "    plt.title('Figure 1: ROC Curves', fontsize=16, weight='bold')\n",
        "    plt.xlabel('False Positive Rate', fontsize=12, weight='bold')\n",
        "    plt.ylabel('True Positive Rate', fontsize=12, weight='bold')\n",
        "    plt.grid(linestyle=':', alpha=0.6)\n",
        "    plt.legend(fontsize=10, loc='lower right')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(output_file, format='svg', bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"Saved to '{output_file}'\")"
      ],
      "metadata": {
        "id": "visualization"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    \"\"\"Phase 1: Engineered vs Perceptual features\"\"\"\n",
        "    OUTPUT_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "    TARGET_VARIABLE, df_orig = load_and_preprocess_data(\n",
        "        INPUT_EXCEL_PATH, TARGET_COLUMN_FOR_LABELING, GROUP_THRESHOLD\n",
        "    )\n",
        "\n",
        "    table1_df = create_patient_characteristics_table(df_orig, TARGET_VARIABLE)\n",
        "    print(table1_df.to_string())\n",
        "\n",
        "    df_processed = df_orig.copy()\n",
        "    y = df_processed[TARGET_VARIABLE]\n",
        "    groups = df_processed['Pt No']\n",
        "\n",
        "    feature_sets = {\n",
        "        \"A-D\": [col for col in df_processed.columns if re.match(r\"^[A-D]\\d+\", col)],\n",
        "        \"E_only\": [col for col in df_processed.columns if re.match(r\"^E\\d+\", col)],\n",
        "    }\n",
        "\n",
        "    numeric_cols = list(set([f for fs in feature_sets.values() for f in fs]))\n",
        "    for col in numeric_cols:\n",
        "        df_processed[col] = pd.to_numeric(df_processed[col], errors='coerce').fillna(\n",
        "            df_processed[col].mean()\n",
        "        )\n",
        "\n",
        "    models = {\n",
        "        \"LR\": Pipeline([\n",
        "            ('scaler', StandardScaler()),\n",
        "            ('model', LogisticRegression(\n",
        "                random_state=UNIFIED_RANDOM_STATE,\n",
        "                class_weight='balanced',\n",
        "                max_iter=1000\n",
        "            ))\n",
        "        ]),\n",
        "        \"RF\": RandomForestClassifier(\n",
        "            random_state=UNIFIED_RANDOM_STATE,\n",
        "            class_weight='balanced'\n",
        "        )\n",
        "    }\n",
        "\n",
        "    train_indices, test_indices = split_data_by_patient(\n",
        "        df_processed, TARGET_VARIABLE, test_size=0.2, random_state=UNIFIED_RANDOM_STATE\n",
        "    )\n",
        "\n",
        "    print(\"\\n--- Selecting stable features using training data only ---\")\n",
        "    df_train = df_processed.loc[train_indices]\n",
        "    y_train = y.loc[train_indices]\n",
        "    groups_train = groups.loc[train_indices]\n",
        "\n",
        "    selected_feature_dfs = {}\n",
        "    for name, f_list in feature_sets.items():\n",
        "        print(f\"\\nFeature set '{name}' stability selection...\")\n",
        "        selected_cols = select_stable_features(\n",
        "            df_train, y_train, groups_train, f_list,\n",
        "            models[\"RF\"], N_FEATURES_TO_SELECT,\n",
        "            N_BOOTSTRAPS_STABILITY, UNIFIED_RANDOM_STATE\n",
        "        )\n",
        "        selected_feature_dfs[name] = df_processed[selected_cols]\n",
        "\n",
        "    final_results = {}\n",
        "    summary_data = []\n",
        "    y_pred_probas_test = {}\n",
        "\n",
        "    print(\"\\n--- Evaluating models ---\")\n",
        "    for model_key, model_pipeline in models.items():\n",
        "        for feature_set_name, X_selected in selected_feature_dfs.items():\n",
        "            model_id = f\"{model_key}_{feature_set_name}\"\n",
        "            print(f\"\\nProcessing: {model_id}\")\n",
        "\n",
        "            cv_perf = evaluate_model_with_cv(\n",
        "                X_selected.loc[train_indices], y_train, groups_train, model_pipeline\n",
        "            )\n",
        "\n",
        "            X_train = X_selected.loc[train_indices]\n",
        "            X_test = X_selected.loc[test_indices]\n",
        "            y_train_loop = y.loc[train_indices]\n",
        "            y_test_loop = y.loc[test_indices]\n",
        "            groups_test = groups.loc[test_indices]\n",
        "\n",
        "            final_model = clone(model_pipeline).fit(X_train, y_train_loop)\n",
        "\n",
        "            proba_test = final_model.predict_proba(X_test)[:, 1]\n",
        "            y_pred_probas_test[model_id] = pd.Series(proba_test, index=X_test.index)\n",
        "\n",
        "            ci_lower, ci_upper = calculate_bootstrap_ci(y_test_loop, proba_test, groups_test)\n",
        "\n",
        "            summary_data.append({\n",
        "                'Model': model_id,\n",
        "                'Features': X_selected.shape[1],\n",
        "                'AUC': f\"{cv_perf.loc['auc', 'mean']:.3f} ± {cv_perf.loc['auc', 'std']:.3f}\",\n",
        "                'AUC (95% CI)': f\"{ci_lower:.3f} - {ci_upper:.3f}\",\n",
        "                'Accuracy': f\"{cv_perf.loc['accuracy', 'mean']:.3f} ± {cv_perf.loc['accuracy', 'std']:.3f}\",\n",
        "                'Sensitivity': f\"{cv_perf.loc['sensitivity', 'mean']:.3f} ± {cv_perf.loc['sensitivity', 'std']:.3f}\",\n",
        "                'Specificity': f\"{cv_perf.loc['specificity', 'mean']:.3f} ± {cv_perf.loc['specificity', 'std']:.3f}\",\n",
        "            })\n",
        "\n",
        "            final_results[model_id] = {\n",
        "                'model': final_model,\n",
        "                'X_test': X_test,\n",
        "                'y_test': y_test_loop,\n",
        "                'X_train': X_train\n",
        "            }\n",
        "\n",
        "    table2_df = pd.DataFrame(summary_data)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"--- Table 2: Model Performance Summary ---\")\n",
        "    print(\"=\"*50)\n",
        "    print(table2_df.to_string(index=False))\n",
        "\n",
        "    plot_roc_curves(\n",
        "        final_results, table2_df,\n",
        "        OUTPUT_DIR / f'Figure1_ROC_Curve_{TARGET_COLUMN_FOR_LABELING}.svg'\n",
        "    )\n",
        "\n",
        "    print(\"\\nPhase 1 analysis complete.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "main_execution"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
