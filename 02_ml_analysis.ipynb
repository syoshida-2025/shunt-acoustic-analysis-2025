{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {"provenance": []},
    "kernelspec": {"name": "python3", "display_name": "Python 3"},
    "language_info": {"name": "python"}
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": ["# AVF Acoustic Analysis: Machine Learning Classification (Full Version)\n\nThis notebook performs:\n- **Phase 1**: Comparison of engineered (A-D) vs. perceptual (E) acoustic features\n- **Phase 2**: Comparison of acoustic-only vs. acoustic + clinical features\n\nMethods:\n- Patient-level data splitting\n- Stability selection with bootstrap (500 iterations)\n- Logistic Regression and Random Forest classifiers\n- SHAP analysis for model interpretability\n- DeLong test for AUC comparison"],
      "metadata": {"id": "header"}
    },
    {
      "cell_type": "code",
      "source": ["import sys\nimport pandas as pd\nimport numpy as np\nimport re\nimport warnings\nimport time\nfrom pathlib import Path\nfrom collections import Counter\n\nif \"google.colab\" in sys.modules:\n    !pip install python-docx statsmodels shap openpyxl -q\n    !apt-get -qq install -y fonts-liberation\n    from google.colab import drive\n    drive.mount('/content/drive')\nelse:\n    print(\"Local environment: packages assumed installed\")\n\nfrom sklearn.model_selection import train_test_split, GroupKFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import (\n    classification_report, roc_auc_score, RocCurveDisplay,\n    brier_score_loss\n)\nfrom sklearn.feature_selection import RFE\nfrom sklearn.base import clone\nfrom sklearn.calibration import CalibrationDisplay\nfrom sklearn.compose import make_column_selector, ColumnTransformer\n\nfrom scipy.stats import mannwhitneyu, chi2_contingency, norm\nimport matplotlib.pyplot as plt\nfrom docx import Document\nfrom statsmodels.stats.multitest import multipletests\n\nwarnings.filterwarnings('ignore')\nplt.rcParams['font.family'] = 'Liberation Sans'"],
      "metadata": {"id": "imports"},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": ["# ==============================================================================\n# Configuration\n# ==============================================================================\nTARGET_COLUMN_FOR_LABELING = 'FV'  # 'FV' or 'RI'\nGROUP_THRESHOLD = 400              # FV: 400, RI: 0.6\n\nDRIVE_BASE_PATH = Path(\"/path/to/your/data/folder/\")\nINPUT_EXCEL_PATH = DRIVE_BASE_PATH / \"your_data.xlsx\"\nOUTPUT_DIR = DRIVE_BASE_PATH / f\"{TARGET_COLUMN_FOR_LABELING}_{GROUP_THRESHOLD}_results\"\n\nN_FEATURES_TO_SELECT = 5\nN_SPLITS_CV = 5\nN_BOOTSTRAPS_STABILITY = 500\nN_BOOTSTRAPS_CI = 1000\nUNIFIED_RANDOM_STATE = 243"],
      "metadata": {"id": "config"},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": ["# ==============================================================================\n# Data Loading & Preprocessing\n# ==============================================================================\ndef load_and_preprocess_data(file_path: Path, target_column: str, threshold: float):\n    if not file_path.exists():\n        raise FileNotFoundError(f\"File not found: {file_path}\")\n\n    df = pd.read_excel(file_path)\n    target_variable = f'{target_column}>={threshold}'\n    df[target_variable] = (df[target_column] >= threshold).astype(int)\n\n    print(f\"Loaded data from '{file_path.name}'\")\n    print(f\"Created binary target '{target_variable}' from '{target_column}'\")\n    print(f\"n={len(df)} samples\")\n\n    return target_variable, df\n\n\ndef split_data_by_patient(df: pd.DataFrame, target_variable: str,\n                          test_size: float = 0.2, random_state: int = None):\n    unique_patients = df['Pt No'].unique()\n    patient_labels = df.drop_duplicates(subset=['Pt No']).set_index('Pt No')[target_variable]\n\n    train_patients, test_patients = train_test_split(\n        unique_patients, test_size=test_size, random_state=random_state,\n        stratify=patient_labels.reindex(unique_patients)\n    )\n\n    train_indices = df[df['Pt No'].isin(train_patients)].index\n    test_indices = df[df['Pt No'].isin(test_patients)].index\n\n    print(f\"\\nPatient-level split: Train={len(train_patients)}, Test={len(test_patients)}\")\n    print(f\"Sample-level split: Train={len(train_indices)}, Test={len(test_indices)}\")\n\n    for name, indices in [(\"Training\", train_indices), (\"Test\", test_indices)]:\n        dist = df.loc[indices, target_variable].value_counts(normalize=True).sort_index()\n        print(f\"\\n{name} data ({len(indices)} samples):\")\n        for label, prop in dist.items():\n            count = df.loc[indices, target_variable].value_counts().get(label, 0)\n            print(f\"  Class {label}: {count} ({prop:.2%})\")\n\n    return train_indices, test_indices"],
      "metadata": {"id": "data_loading"},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": ["# ==============================================================================\n# Table Creation Functions\n# ==============================================================================\ndef create_patient_characteristics_table(df: pd.DataFrame, target_variable: str) -> pd.DataFrame:\n    print(\"\\n--- Creating Table 1: Patient Characteristics ---\")\n    df_patients = df.drop_duplicates(subset=['Pt No']).copy()\n\n    group0 = df_patients[df_patients[target_variable] == 0]\n    group1 = df_patients[df_patients[target_variable] == 1]\n\n    table_data = []\n\n    continuous_vars = ['age', 'POD', 'FV', 'RI', 'RA diameter', 'shunt diameter']\n    for var in continuous_vars:\n        if var not in df.columns:\n            continue\n\n        n_missing0 = group0[var].isnull().sum()\n        n_missing1 = group1[var].isnull().sum()\n\n        mean0, std0 = group0[var].mean(), group0[var].std()\n        mean1, std1 = group1[var].mean(), group1[var].std()\n\n        mean0_str = f\"{mean0:.1f} ± {std0:.1f}\"\n        if n_missing0 > 0:\n            mean0_str += f\" (missing: {n_missing0})\"\n\n        mean1_str = f\"{mean1:.1f} ± {std1:.1f}\"\n        if n_missing1 > 0:\n            mean1_str += f\" (missing: {n_missing1})\"\n\n        _, p_val = mannwhitneyu(group0[var].dropna(), group1[var].dropna())\n\n        table_data.append({\n            'Characteristic': var,\n            f'{TARGET_COLUMN_FOR_LABELING} < {GROUP_THRESHOLD} (n={len(group0)})': mean0_str,\n            f'{TARGET_COLUMN_FOR_LABELING} >= {GROUP_THRESHOLD} (n={len(group1)})': mean1_str,\n            'P-Value': f\"{p_val:.2f}\" if p_val >= 0.01 else \"<0.01\"\n        })\n\n    categorical_vars = {\n        'men': 'Male Sex (n, %)',\n        'arterial calcification': 'Arterial Calcification (n, %)',\n        'DM': 'DM (n, %)',\n        'HTN': 'HTN (n, %)',\n        'Heart disease': 'Heart Disease (n, %)',\n        'tabaco site': 'tabaco site (n, %)',\n        'left': 'left (n, %)'\n    }\n\n    mapping_dict = {'y': 1, 'n': 0, 'male': 1, 'female': 0, '1': 1, '0': 0, '1.0': 1, '0.0': 0}\n\n    for var, name in categorical_vars.items():\n        if var not in df_patients.columns:\n            continue\n\n        df_patients[var + '_numeric'] = df_patients[var].astype(str).str.lower().map(mapping_dict)\n        df_filtered = df_patients.dropna(subset=[var + '_numeric'])\n\n        group0_cat = df_filtered[df_filtered[target_variable] == 0]\n        group1_cat = df_filtered[df_filtered[target_variable] == 1]\n\n        crosstab = pd.crosstab(df_filtered[var + '_numeric'], df_filtered[target_variable])\n\n        p_val = chi2_contingency(crosstab)[1] if crosstab.shape == (2, 2) else 1.0\n\n        n0_pos = crosstab.loc[1, 0] if 1 in crosstab.index and 0 in crosstab.columns else 0\n        n1_pos = crosstab.loc[1, 1] if 1 in crosstab.index and 1 in crosstab.columns else 0\n\n        percent0 = (n0_pos / len(group0_cat) * 100) if len(group0_cat) > 0 else 0\n        percent1 = (n1_pos / len(group1_cat) * 100) if len(group1_cat) > 0 else 0\n\n        table_data.append({\n            'Characteristic': name,\n            f'{TARGET_COLUMN_FOR_LABELING} < {GROUP_THRESHOLD} (n={len(group0)})': f\"{n0_pos} ({percent0:.1f})\",\n            f'{TARGET_COLUMN_FOR_LABELING} >= {GROUP_THRESHOLD} (n={len(group1)})': f\"{n1_pos} ({percent1:.1f})\",\n            'P-Value': f\"{p_val:.2f}\" if p_val >= 0.01 else \"<0.01\"\n        })\n\n    return pd.DataFrame(table_data)\n\n\ndef create_split_characteristics_table(df: pd.DataFrame, train_indices, test_indices) -> pd.DataFrame:\n    print(\"\\n--- Supplementary Table 1: Train vs. Test Set Characteristics ---\")\n\n    df_train = df.loc[train_indices].drop_duplicates(subset=['Pt No']).copy()\n    df_test = df.loc[test_indices].drop_duplicates(subset=['Pt No']).copy()\n\n    table_data = []\n\n    continuous_vars = ['age', 'POD', 'FV', 'RI', 'RA diameter', 'shunt diameter']\n    for var in continuous_vars:\n        if var not in df.columns:\n            continue\n\n        train_vals = df_train[var].dropna()\n        test_vals = df_test[var].dropna()\n\n        mean_train, std_train = train_vals.mean(), train_vals.std()\n        mean_test, std_test = test_vals.mean(), test_vals.std()\n\n        p_val = mannwhitneyu(train_vals, test_vals)[1] if len(train_vals) > 1 and len(test_vals) > 1 else np.nan\n\n        table_data.append({\n            'Characteristic': var,\n            f'Training Set (n={len(df_train)})': f\"{mean_train:.1f} ± {std_train:.1f}\",\n            f'Test Set (n={len(df_test)})': f\"{mean_test:.1f} ± {std_test:.1f}\",\n            'P-Value': f\"{p_val:.2f}\" if not pd.isna(p_val) else \"N/A\"\n        })\n\n    categorical_vars = {\n        'men': 'Male Sex (n, %)',\n        'arterial calcification': 'Arterial Calcification (n, %)',\n        'DM': 'DM (n, %)',\n        'HTN': 'HTN (n, %)',\n        'Heart disease': 'Heart Disease (n, %)',\n        'tabaco site': 'tabaco site (n, %)',\n        'left': 'left (n, %)'\n    }\n\n    mapping_dict = {'y': 1, 'n': 0, 'male': 1, 'female': 0, '1': 1, '0': 0, '1.0': 1, '0.0': 0}\n\n    df_unique = df.drop_duplicates(subset=['Pt No']).copy()\n    df_unique['set'] = np.where(df_unique.index.isin(train_indices), 'train', 'test')\n\n    for var, name in categorical_vars.items():\n        if var not in df.columns:\n            continue\n\n        df_unique[var + '_numeric'] = df_unique[var].astype(str).str.lower().map(mapping_dict)\n        df_filtered = df_unique.dropna(subset=[var + '_numeric'])\n\n        crosstab = pd.crosstab(df_filtered[var + '_numeric'], df_filtered['set'])\n\n        p_val = chi2_contingency(crosstab)[1] if crosstab.shape[0] >= 2 and crosstab.shape[1] >= 2 else 1.0\n\n        n_train_pos = crosstab.get('train', pd.Series(0)).get(1, 0)\n        n_test_pos = crosstab.get('test', pd.Series(0)).get(1, 0)\n\n        p_train = (n_train_pos / len(df_train) * 100) if len(df_train) > 0 else 0\n        p_test = (n_test_pos / len(df_test) * 100) if len(df_test) > 0 else 0\n\n        table_data.append({\n            'Characteristic': name,\n            f'Training Set (n={len(df_train)})': f\"{n_train_pos} ({p_train:.1f})\",\n            f'Test Set (n={len(df_test)})': f\"{n_test_pos} ({p_test:.1f})\",\n            'P-Value': f\"{p_val:.2f}\"\n        })\n\n    return pd.DataFrame(table_data)\n\n\ndef print_patient_ids_summary(df: pd.DataFrame, train_indices, test_indices, target_variable: str, output_dir: Path):\n    print(\"\\n\" + \"=\"*70)\n    print(\"--- Patient ID Summary ---\")\n    print(\"=\"*70)\n\n    df_train = df.loc[train_indices].drop_duplicates(subset=['Pt No'])\n    train_class0 = df_train[df_train[target_variable] == 0]['Pt No'].sort_values().tolist()\n    train_class1 = df_train[df_train[target_variable] == 1]['Pt No'].sort_values().tolist()\n\n    print(f\"\\nTraining data:\")\n    print(f\"  {TARGET_COLUMN_FOR_LABELING} < {GROUP_THRESHOLD} (n={len(train_class0)}): {train_class0}\")\n    print(f\"  {TARGET_COLUMN_FOR_LABELING} >= {GROUP_THRESHOLD} (n={len(train_class1)}): {train_class1}\")\n\n    df_test = df.loc[test_indices].drop_duplicates(subset=['Pt No'])\n    test_class0 = df_test[df_test[target_variable] == 0]['Pt No'].sort_values().tolist()\n    test_class1 = df_test[df_test[target_variable] == 1]['Pt No'].sort_values().tolist()\n\n    print(f\"\\nTest data:\")\n    print(f\"  {TARGET_COLUMN_FOR_LABELING} < {GROUP_THRESHOLD} (n={len(test_class0)}): {test_class0}\")\n    print(f\"  {TARGET_COLUMN_FOR_LABELING} >= {GROUP_THRESHOLD} (n={len(test_class1)}): {test_class1}\")\n\n    output_path = output_dir / \"patient_ids_summary.csv\"\n    summary_df = pd.DataFrame({\n        'Dataset': ['Train', 'Train', 'Test', 'Test'],\n        'Class': [f'< {GROUP_THRESHOLD}', f'>= {GROUP_THRESHOLD}', f'< {GROUP_THRESHOLD}', f'>= {GROUP_THRESHOLD}'],\n        'Count': [len(train_class0), len(train_class1), len(test_class0), len(test_class1)],\n        'Patient_IDs': [', '.join(map(str, train_class0)), ', '.join(map(str, train_class1)),\n                       ', '.join(map(str, test_class0)), ', '.join(map(str, test_class1))]\n    })\n    summary_df.to_csv(output_path, index=False, encoding='utf-8-sig')\n    print(f\"\\nSaved patient IDs to '{output_path}'\")\n    print(\"=\"*70)"],
      "metadata": {"id": "table_functions"},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": ["# ==============================================================================\n# Feature Selection\n# ==============================================================================\ndef select_stable_features(X_train_df: pd.DataFrame, y_train: pd.Series,\n                          groups_train: pd.Series, feature_list: list,\n                          model_for_select, n_features_to_select: int,\n                          n_bootstraps: int, random_state: int) -> list:\n    print(f\"-> Starting stability selection (bootstrap iterations: {n_bootstraps})...\")\n\n    feature_counter = Counter()\n    unique_patients = groups_train.unique()\n    n_patients = len(unique_patients)\n    rng = np.random.RandomState(random_state)\n\n    if isinstance(model_for_select, Pipeline):\n        scaler = clone(model_for_select.steps[0][1])\n        estimator = clone(model_for_select.steps[-1][1])\n    else:\n        scaler = None\n        estimator = clone(model_for_select)\n\n    selector = RFE(estimator=estimator, n_features_to_select=n_features_to_select, step=1)\n    X_train_subset = X_train_df[feature_list]\n\n    valid_iterations = 0\n    for i in range(n_bootstraps):\n        if (i + 1) % 100 == 0:\n            print(f\"  ... Bootstrap {i+1}/{n_bootstraps} complete\")\n\n        try:\n            sampled_patients = rng.choice(unique_patients, size=n_patients, replace=True)\n            mask = groups_train.isin(sampled_patients)\n            idx = groups_train[mask].index\n\n            if len(idx) == 0 or len(np.unique(y_train.loc[idx])) < 2:\n                continue\n\n            X_boot = X_train_subset.loc[idx]\n            y_boot = y_train.loc[idx]\n\n            X_boot_scaled = scaler.fit_transform(X_boot) if scaler else X_boot\n\n            selector.fit(X_boot_scaled, y_boot)\n            selected = X_boot.columns[selector.support_].tolist()\n\n            feature_counter.update(selected)\n            valid_iterations += 1\n\n        except Exception:\n            continue\n\n    if valid_iterations == 0:\n        print(\"Warning: No valid bootstrap iterations.\")\n        return feature_list[:n_features_to_select]\n\n    most_common = [f for f, _ in feature_counter.most_common(n_features_to_select)]\n\n    print(f\"-> Stability selection complete. ({valid_iterations}/{n_bootstraps} valid iterations)\")\n    print(\"--- Selected features (by frequency) ---\")\n    for feature, count in feature_counter.most_common(10):\n        print(f\"  {feature}: {count} ({count/valid_iterations:.1%})\")\n\n    return most_common"],
      "metadata": {"id": "feature_selection"},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": ["# ==============================================================================\n# Model Evaluation\n# ==============================================================================\ndef evaluate_model_with_cv(X_selected: pd.DataFrame, y: pd.Series,\n                          groups: pd.Series, model) -> pd.DataFrame:\n    group_kfold = GroupKFold(n_splits=N_SPLITS_CV)\n    scores = {\n        'auc': [], 'accuracy': [], 'sensitivity': [],\n        'specificity': [], 'f1_score': []\n    }\n\n    for train_idx, test_idx in group_kfold.split(X_selected, y, groups):\n        X_train_cv = X_selected.iloc[train_idx]\n        X_test_cv = X_selected.iloc[test_idx]\n        y_train_cv = y.iloc[train_idx]\n        y_test_cv = y.iloc[test_idx]\n\n        model_clone = clone(model)\n        model_clone.fit(X_train_cv, y_train_cv)\n\n        y_pred_proba = model_clone.predict_proba(X_test_cv)[:, 1]\n        y_pred = model_clone.predict(X_test_cv)\n\n        if len(np.unique(y_test_cv)) > 1:\n            scores['auc'].append(roc_auc_score(y_test_cv, y_pred_proba))\n        else:\n            scores['auc'].append(np.nan)\n\n        report = classification_report(y_test_cv, y_pred, output_dict=True, zero_division=0)\n        scores['accuracy'].append(report['accuracy'])\n        scores['sensitivity'].append(report.get('1', {}).get('recall', np.nan))\n        scores['specificity'].append(report.get('0', {}).get('recall', np.nan))\n        scores['f1_score'].append(report['weighted avg']['f1-score'])\n\n    return pd.DataFrame(scores).agg(['mean', 'std']).T\n\n\ndef calculate_bootstrap_ci(y_test: pd.Series, y_pred_proba: np.ndarray,\n                          test_patient_ids: pd.Series) -> tuple:\n    unique_patients = test_patient_ids.unique()\n    n_patients = len(unique_patients)\n    rng = np.random.RandomState(UNIFIED_RANDOM_STATE)\n\n    bootstrapped_scores = []\n    for _ in range(N_BOOTSTRAPS_CI):\n        sampled_patients = rng.choice(unique_patients, size=n_patients, replace=True)\n        mask = test_patient_ids.isin(sampled_patients)\n        idx = test_patient_ids[mask].index\n\n        if len(idx) == 0:\n            continue\n\n        y_true_bs = y_test.loc[idx]\n        idx_positions = y_test.index.get_indexer(idx)\n        y_pred_bs = y_pred_proba[idx_positions]\n\n        if len(np.unique(y_true_bs)) < 2:\n            continue\n\n        bootstrapped_scores.append(roc_auc_score(y_true_bs, y_pred_bs))\n\n    if not bootstrapped_scores:\n        return np.nan, np.nan\n\n    return np.percentile(bootstrapped_scores, 2.5), np.percentile(bootstrapped_scores, 97.5)\n\n\ndef get_feature_importances(model, columns: list) -> pd.Series:\n    estimator = model.steps[-1][1] if isinstance(model, Pipeline) else model\n\n    if hasattr(estimator, 'feature_importances_'):\n        importances = estimator.feature_importances_\n    elif hasattr(estimator, 'coef_'):\n        importances = np.abs(estimator.coef_[0])\n    else:\n        importances = np.zeros(len(columns))\n\n    return pd.Series(importances, index=columns).sort_values(ascending=False)\n\n\ndef display_test_auc_summary(final_results: dict, y_pred_probas_test: dict,\n                             y_test_global: pd.Series, test_patient_ids_global: pd.Series,\n                             phase_name: str = \"Phase 1\"):\n    print(\"\\n\" + \"=\"*70)\n    print(f\"### {phase_name}: Test AUC Summary ###\")\n    print(\"=\"*70)\n\n    results = []\n    for model_id in final_results.keys():\n        if model_id not in y_pred_probas_test:\n            continue\n\n        proba = y_pred_probas_test[model_id].values\n        test_auc = roc_auc_score(y_test_global, proba)\n        ci_lower, ci_upper = calculate_bootstrap_ci(y_test_global, proba, test_patient_ids_global)\n\n        results.append({\n            'Model': model_id,\n            'Test_AUC': test_auc,\n            'CI_Lower': ci_lower,\n            'CI_Upper': ci_upper\n        })\n\n        print(f\"\\n{model_id}:\")\n        print(f\"  Test AUC: {test_auc:.4f} [{ci_lower:.4f}-{ci_upper:.4f}]\")\n\n    if results:\n        results_df = pd.DataFrame(results)\n        print(\"\\n\" + \"=\"*70)\n        print(\"Formatted for Table:\")\n        print(\"=\"*70)\n        for _, row in results_df.iterrows():\n            print(f\"{row['Model']:30s}: Test AUC = {row['Test_AUC']:.3f} [{row['CI_Lower']:.3f}-{row['CI_Upper']:.3f}]\")\n\n    print(\"=\"*70)\n    return results"],
      "metadata": {"id": "model_evaluation"},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": ["# ==============================================================================\n# AUC Comparison (DeLong Test)\n# ==============================================================================\ndef compute_midrank(x):\n    J = np.argsort(x)\n    Z = x[J]\n    N = len(x)\n    T = np.zeros(N, dtype=float)\n    i = 0\n    while i < N:\n        j = i\n        while j < N and Z[j] == Z[i]:\n            j += 1\n        T[i:j] = 0.5 * (i + j - 1)\n        i = j\n    T2 = np.empty(N, dtype=float)\n    T2[J] = T + 1\n    return T2\n\n\ndef fast_delong_auc_cov(y_true, y_pred):\n    y_true = np.asarray(y_true)\n    y_pred = np.asarray(y_pred)\n\n    m = sum(y_true == 1)\n    n = sum(y_true == 0)\n\n    v_10 = compute_midrank(y_pred[y_true == 1])\n    v_01 = compute_midrank(y_pred[y_true == 0])\n\n    auc = (np.sum(v_10) - m * (m + 1) / 2) / (m * n)\n\n    s_10 = v_10 / m - (m + 1) / (2 * m)\n    s_01 = v_01 / n - (n + 1) / (2 * n)\n\n    var_10 = (np.sum(s_10**2) - m * (s_10.sum()/m)**2) / (m - 1)\n    var_01 = (np.sum(s_01**2) - n * (s_01.sum()/n)**2) / (n - 1)\n\n    return auc, var_10, var_01\n\n\ndef delong_roc_test(y_true, scores_a, scores_b):\n    y_true = np.asarray(y_true)\n    scores_a = np.asarray(scores_a)\n    scores_b = np.asarray(scores_b)\n\n    m = sum(y_true == 1)\n    n = sum(y_true == 0)\n\n    if m == 0 or n == 0:\n        return 1.0\n\n    auc_a, var_a_10, var_a_01 = fast_delong_auc_cov(y_true, scores_a)\n    auc_b, var_b_10, var_b_01 = fast_delong_auc_cov(y_true, scores_b)\n\n    v_a_10 = compute_midrank(scores_a[y_true == 1])\n    v_a_01 = compute_midrank(scores_a[y_true == 0])\n    v_b_10 = compute_midrank(scores_b[y_true == 1])\n    v_b_01 = compute_midrank(scores_b[y_true == 0])\n\n    s_a_10 = v_a_10 / m - (m + 1) / (2 * m)\n    s_a_01 = v_a_01 / n - (n + 1) / (2 * n)\n    s_b_10 = v_b_10 / m - (m + 1) / (2 * m)\n    s_b_01 = v_b_01 / n - (n + 1) / (2 * n)\n\n    cov_10 = (np.sum(s_a_10 * s_b_10) - m * (s_a_10.sum()/m) * (s_b_10.sum()/m)) / (m - 1)\n    cov_01 = (np.sum(s_a_01 * s_b_01) - n * (s_a_01.sum()/n) * (s_b_01.sum()/n)) / (n - 1)\n\n    var_diff = var_a_10 / m + var_a_01 / n + var_b_10 / m + var_b_01 / n - 2 * (cov_10 / m + cov_01 / n)\n\n    if var_diff <= 1e-8:\n        return 1.0\n\n    z = (auc_a - auc_b) / np.sqrt(var_diff)\n    p = 2 * norm.sf(np.abs(z))\n\n    return p\n\n\ndef compute_auc_diff_paired(y_test: pd.Series, y_pred_probas: pd.DataFrame,\n                           test_patient_ids: pd.Series, model_pair: tuple) -> dict:\n    model_a, model_b = model_pair\n    proba_a = y_pred_probas[model_a]\n    proba_b = y_pred_probas[model_b]\n\n    unique_patients = test_patient_ids.unique()\n    n_patients = len(unique_patients)\n    rng = np.random.RandomState(UNIFIED_RANDOM_STATE)\n\n    auc_diffs = []\n    for _ in range(N_BOOTSTRAPS_CI):\n        sampled_patients = rng.choice(unique_patients, size=n_patients, replace=True)\n        mask = test_patient_ids.isin(sampled_patients)\n        idx = test_patient_ids[mask].index\n\n        if len(idx) == 0:\n            continue\n\n        y_true_bs = y_test.loc[idx]\n\n        if len(np.unique(y_true_bs)) < 2:\n            continue\n\n        auc_a = roc_auc_score(y_true_bs, proba_a.loc[idx])\n        auc_b = roc_auc_score(y_true_bs, proba_b.loc[idx])\n        auc_diffs.append(auc_a - auc_b)\n\n    if not auc_diffs:\n        return {\n            'model_pair': f\"{model_a} vs {model_b}\",\n            'auc_diff_mean': np.nan,\n            'ci_lower': np.nan,\n            'ci_upper': np.nan,\n            'p_value': np.nan\n        }\n\n    auc_diffs = np.array(auc_diffs)\n    mean_diff = auc_diffs.mean()\n    ci_lower = np.percentile(auc_diffs, 2.5)\n    ci_upper = np.percentile(auc_diffs, 97.5)\n\n    p_val = min(np.mean(auc_diffs <= 0), np.mean(auc_diffs >= 0)) * 2\n    p_val = min(p_val, 1.0)\n\n    return {\n        'model_pair': f\"{model_a} vs {model_b}\",\n        'auc_diff_mean': mean_diff,\n        'ci_lower': ci_lower,\n        'ci_upper': ci_upper,\n        'p_value': p_val\n    }"],
      "metadata": {"id": "auc_comparison"},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": ["# ==============================================================================\n# Visualization\n# ==============================================================================\ndef analyze_and_plot_shap(model, X_train, X_test, output_dir, model_name, target_name):\n    import shap\n\n    print(\"\\n\" + \"=\"*70)\n    print(\"--- SHAP Analysis ---\")\n    print(\"=\"*70)\n\n    plt.rcParams['font.family'] = 'Liberation Sans'\n\n    model_to_explain = model.steps[-1][1] if isinstance(model, Pipeline) else model\n    print(f\"Initializing SHAP explainer for: {type(model_to_explain).__name__}\")\n\n    explainer = shap.Explainer(model_to_explain, X_train)\n    explanation = explainer(X_test)\n\n    explanation_plot = explanation[:, :, 1] if hasattr(explanation, 'values') and explanation.values.ndim == 3 else explanation\n\n    print(\"Generating SHAP Summary Plot...\")\n    plt.figure(figsize=(10, 8))\n    shap.summary_plot(explanation_plot, features=X_test, show=False, plot_size=None)\n    fig_summary = output_dir / f'SHAP_Summary_{target_name}.svg'\n    plt.title(f'SHAP Summary Plot ({model_name})', fontsize=16, weight='bold')\n    plt.tight_layout()\n    plt.savefig(fig_summary, format='svg', bbox_inches='tight')\n    plt.show()\n    print(f\"Saved to '{fig_summary}'\")\n\n    return [fig_summary]\n\n\ndef plot_calibration_curve(model, X_test, y_test, model_name, output_dir, target_name):\n    print(\"\\n\" + \"=\"*70)\n    print(\"--- Calibration Analysis ---\")\n    print(\"=\"*70)\n\n    y_pred_proba = model.predict_proba(X_test)[:, 1]\n    brier = brier_score_loss(y_test, y_pred_proba)\n\n    print(f\"Brier Score for '{model_name}': {brier:.4f}\")\n    print(\"(Brier score closer to 0 indicates better calibration)\")\n\n    fig, ax = plt.subplots(figsize=(10, 8))\n    CalibrationDisplay.from_predictions(\n        y_test, y_pred_proba, n_bins=5, name=model_name,\n        ax=ax, strategy='uniform'\n    )\n\n    ax.set_title(f'Calibration Plot ({model_name})', fontsize=16, weight='bold')\n    ax.set_xlabel(\"Mean Predicted Probability\", fontsize=12, weight='bold')\n    ax.set_ylabel(\"Fraction of Positives\", fontsize=12, weight='bold')\n    ax.grid(linestyle=':', alpha=0.6)\n\n    fig_path = output_dir / f'Calibration_{target_name}.svg'\n    plt.tight_layout()\n    plt.savefig(fig_path, format='svg', bbox_inches='tight')\n    plt.show()\n\n    print(f\"Saved to '{fig_path}'\")\n    return fig_path\n\n\ndef plot_roc_curves(final_results: dict, table2_df: pd.DataFrame, output_file: Path):\n    plt.figure(figsize=(12, 10))\n    ax = plt.gca()\n\n    for model_id in table2_df['Model'].unique():\n        if model_id not in final_results:\n            continue\n\n        result = final_results[model_id]\n        cv_auc = table2_df.loc[table2_df['Model'] == model_id, 'AUC'].values[0].split(' ±')[0]\n        label = f\"{model_id} (AUC = {float(cv_auc):.3f})\"\n\n        RocCurveDisplay.from_estimator(\n            result['model'], result['X_test'], result['y_test'],\n            name=label, ax=ax\n        )\n\n    plt.plot([0, 1], [0, 1], color='black', lw=1, linestyle='--')\n    plt.title('ROC Curves', fontsize=16, weight='bold')\n    plt.xlabel('False Positive Rate (1 - Specificity)', fontsize=12, weight='bold')\n    plt.ylabel('True Positive Rate (Sensitivity)', fontsize=12, weight='bold')\n    plt.grid(linestyle=':', alpha=0.6)\n    plt.legend(fontsize=10, loc='lower right')\n    plt.tight_layout()\n    plt.savefig(output_file, format='svg', bbox_inches='tight')\n    plt.show()\n\n    print(f\"Saved to '{output_file}'\")\n\n\ndef save_tables_to_word(table1_df, supp_table1_df, table2_df, table3_df, table4_data, output_file, target_variable):\n    doc = Document()\n\n    doc.add_heading(f'Table 1: Patient Characteristics ({target_variable})', level=1)\n    t1 = doc.add_table(table1_df.shape[0] + 1, table1_df.shape[1], style='Table Grid')\n    for j, col in enumerate(table1_df.columns):\n        t1.cell(0, j).text = col\n    for i in range(table1_df.shape[0]):\n        for j in range(table1_df.shape[1]):\n            t1.cell(i + 1, j).text = str(table1_df.iloc[i, j])\n    doc.add_page_break()\n\n    if supp_table1_df is not None and not supp_table1_df.empty:\n        doc.add_heading('Supplementary Table 1: Train vs. Test Set Characteristics', level=1)\n        ts1 = doc.add_table(supp_table1_df.shape[0] + 1, supp_table1_df.shape[1], style='Table Grid')\n        for j, col in enumerate(supp_table1_df.columns):\n            ts1.cell(0, j).text = col\n        for i in range(supp_table1_df.shape[0]):\n            for j in range(supp_table1_df.shape[1]):\n                ts1.cell(i + 1, j).text = str(supp_table1_df.iloc[i, j])\n        doc.add_page_break()\n\n    doc.add_heading(f'Table 2: Model Performance Summary ({target_variable})', level=1)\n    t2 = doc.add_table(table2_df.shape[0] + 1, table2_df.shape[1], style='Table Grid')\n    for j, col in enumerate(table2_df.columns):\n        t2.cell(0, j).text = col\n    for i in range(table2_df.shape[0]):\n        for j in range(table2_df.shape[1]):\n            t2.cell(i + 1, j).text = str(table2_df.iloc[i, j])\n    doc.add_page_break()\n\n    if table3_df is not None and not table3_df.empty:\n        doc.add_heading(f'Table 3: Feature Importances ({target_variable})', level=1)\n        table3_reset = table3_df.reset_index()\n        t3 = doc.add_table(table3_reset.shape[0] + 1, table3_reset.shape[1], style='Table Grid')\n        for j, col in enumerate(table3_reset.columns):\n            t3.cell(0, j).text = col\n        for i in range(table3_reset.shape[0]):\n            for j in range(table3_reset.shape[1]):\n                val = table3_reset.iloc[i, j]\n                t3.cell(i + 1, j).text = f\"{val:.4f}\" if isinstance(val, (float, np.floating)) else str(val)\n        doc.add_page_break()\n\n    doc.add_heading('Table 4: AUC Difference Comparisons', level=1)\n    if not table4_data:\n        doc.add_paragraph(\"No AUC difference comparisons were performed.\")\n    else:\n        for idx, (prefix, df) in enumerate(table4_data.items()):\n            sub_label = chr(97 + idx)\n            doc.add_heading(f'Table 4{sub_label}: {prefix} Model Comparisons', level=2)\n            if df.empty:\n                doc.add_paragraph(\"No data available.\")\n                continue\n            t4 = doc.add_table(df.shape[0] + 1, df.shape[1], style='Table Grid')\n            for j, col in enumerate(df.columns):\n                t4.cell(0, j).text = col\n            for i in range(df.shape[0]):\n                for j in range(df.shape[1]):\n                    t4.cell(i + 1, j).text = str(df.iloc[i, j])\n            doc.add_paragraph()\n\n    doc.save(output_file)\n    print(f\"\\nSaved tables to '{output_file}'\")\n"],
      "metadata": {"id": "visualization"},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": ["# ==============================================================================\n# Phase 1: Engineered vs Perceptual Features\n# ==============================================================================\ndef main_phase1():\n    print(\"\\n\" + \"=\"*80)\n    print(\"### STARTING PHASE 1 ANALYSIS ###\")\n    print(\"### Engineered (A-D) vs Perceptual (E) Features ###\")\n    print(\"=\"*80)\n\n    OUTPUT_DIR.mkdir(exist_ok=True)\n\n    TARGET_VARIABLE, df_orig = load_and_preprocess_data(\n        INPUT_EXCEL_PATH, TARGET_COLUMN_FOR_LABELING, GROUP_THRESHOLD\n    )\n\n    table1_df = create_patient_characteristics_table(df_orig, TARGET_VARIABLE)\n    print(table1_df.to_string())\n\n    df_processed = df_orig.copy()\n    y = df_processed[TARGET_VARIABLE]\n    groups = df_processed['Pt No']\n\n    feature_sets = {\n        \"A-D\": [col for col in df_processed.columns if re.match(r\"^[A-D]\\d+\", col)],\n        \"E_only\": [col for col in df_processed.columns if re.match(r\"^E\\d+\", col)],\n    }\n\n    numeric_cols = list(set([f for fs in feature_sets.values() for f in fs]))\n    for col in numeric_cols:\n        df_processed[col] = pd.to_numeric(df_processed[col], errors='coerce').fillna(\n            df_processed[col].mean()\n        )\n\n    models = {\n        \"LR\": Pipeline([\n            ('scaler', StandardScaler()),\n            ('model', LogisticRegression(\n                random_state=UNIFIED_RANDOM_STATE,\n                class_weight='balanced',\n                max_iter=1000\n            ))\n        ]),\n        \"RF\": RandomForestClassifier(\n            random_state=UNIFIED_RANDOM_STATE,\n            class_weight='balanced'\n        )\n    }\n\n    train_indices, test_indices = split_data_by_patient(\n        df_processed, TARGET_VARIABLE, test_size=0.2, random_state=UNIFIED_RANDOM_STATE\n    )\n\n    supp_table1_df = create_split_characteristics_table(df_orig, train_indices, test_indices)\n    print(supp_table1_df.to_string())\n\n    print_patient_ids_summary(df_processed, train_indices, test_indices, TARGET_VARIABLE, OUTPUT_DIR)\n\n    print(\"\\n--- Selecting stable features using training data only ---\")\n    df_train = df_processed.loc[train_indices]\n    y_train = y.loc[train_indices]\n    groups_train = groups.loc[train_indices]\n\n    selected_feature_dfs = {}\n    for name, f_list in feature_sets.items():\n        print(f\"\\nFeature set '{name}' stability selection...\")\n        selected_cols = select_stable_features(\n            df_train, y_train, groups_train, f_list,\n            models[\"RF\"], N_FEATURES_TO_SELECT,\n            N_BOOTSTRAPS_STABILITY, UNIFIED_RANDOM_STATE\n        )\n        selected_feature_dfs[name] = df_processed[selected_cols]\n\n    final_results = {}\n    summary_data = []\n    table3_data = []\n    y_pred_probas_test = {}\n\n    print(\"\\n--- Evaluating models ---\")\n    for model_key, model_pipeline in models.items():\n        for feature_set_name, X_selected in selected_feature_dfs.items():\n            model_id = f\"{model_key}_{feature_set_name}\"\n            print(f\"\\nProcessing: {model_id}\")\n\n            cv_perf = evaluate_model_with_cv(\n                X_selected.loc[train_indices], y_train, groups_train, model_pipeline\n            )\n\n            X_train = X_selected.loc[train_indices]\n            X_test = X_selected.loc[test_indices]\n            y_train_loop = y.loc[train_indices]\n            y_test_loop = y.loc[test_indices]\n            groups_test = groups.loc[test_indices]\n\n            model_for_importance = clone(model_pipeline).fit(X_train, y_train_loop)\n            final_model = clone(model_pipeline).fit(X_train, y_train_loop)\n\n            proba_test = final_model.predict_proba(X_test)[:, 1]\n            y_pred_probas_test[model_id] = pd.Series(proba_test, index=X_test.index)\n\n            ci_lower, ci_upper = calculate_bootstrap_ci(y_test_loop, proba_test, groups_test)\n\n            summary_data.append({\n                'Model': model_id,\n                'Features': X_selected.shape[1],\n                'AUC': f\"{cv_perf.loc['auc', 'mean']:.3f} ± {cv_perf.loc['auc', 'std']:.3f}\",\n                'AUC (95% CI)': f\"{ci_lower:.3f} - {ci_upper:.3f}\",\n                'Accuracy': f\"{cv_perf.loc['accuracy', 'mean']:.3f} ± {cv_perf.loc['accuracy', 'std']:.3f}\",\n                'Sensitivity': f\"{cv_perf.loc['sensitivity', 'mean']:.3f} ± {cv_perf.loc['sensitivity', 'std']:.3f}\",\n                'Specificity': f\"{cv_perf.loc['specificity', 'mean']:.3f} ± {cv_perf.loc['specificity', 'std']:.3f}\",\n                'F1-Score': f\"{cv_perf.loc['f1_score', 'mean']:.3f} ± {cv_perf.loc['f1_score', 'std']:.3f}\"\n            })\n\n            final_results[model_id] = {\n                'model': final_model,\n                'X_test': X_test,\n                'y_test': y_test_loop,\n                'X_train': X_train\n            }\n\n            importances = get_feature_importances(model_for_importance, X_selected.columns)\n            for feature, importance in importances.items():\n                table3_data.append({\n                    'Model_ID': model_id,\n                    'Feature': feature,\n                    'Importance': importance\n                })\n\n    table2_df = pd.DataFrame(summary_data)\n    table3_df = pd.DataFrame(table3_data).pivot_table(\n        index='Feature', columns='Model_ID', values='Importance'\n    ).sort_index()\n\n    print(\"\\n\" + \"=\"*50)\n    print(\"--- Table 2: Model Performance Summary ---\")\n    print(\"=\"*50)\n    print(table2_df.to_string(index=False))\n\n    print(\"\\n\" + \"=\"*50)\n    print(\"--- Table 3: Top Features per Model ---\")\n    print(\"=\"*50)\n    print(table3_df)\n\n    y_test_global = y.loc[test_indices]\n    test_patient_ids_global = groups.loc[test_indices]\n\n    display_test_auc_summary(\n        final_results, y_pred_probas_test,\n        y_test_global, test_patient_ids_global, \"Phase 1\"\n    )\n\n    print(\"\\n--- Detailed Analysis for Best Model (Phase 1) ---\")\n    model_id_to_analyze = \"LR_E_only\"\n\n    if model_id_to_analyze in final_results:\n        results = final_results[model_id_to_analyze]\n        analyze_and_plot_shap(\n            results['model'], results['X_train'], results['X_test'],\n            OUTPUT_DIR, model_id_to_analyze, f\"{TARGET_COLUMN_FOR_LABELING}_Phase1\"\n        )\n        plot_calibration_curve(\n            results['model'], results['X_test'], results['y_test'],\n            model_id_to_analyze, OUTPUT_DIR, f\"{TARGET_COLUMN_FOR_LABELING}_Phase1\"\n        )\n\n    y_pred_probas_df = pd.DataFrame(y_pred_probas_test)\n    table4_subtables = {}\n\n    for prefix in models.keys():\n        pair = (f'{prefix}_E_only', f'{prefix}_A-D')\n\n        if all(p in y_pred_probas_df.columns for p in pair):\n            res = compute_auc_diff_paired(y_test_global, y_pred_probas_df, test_patient_ids_global, pair)\n            p_delong = delong_roc_test(\n                y_test_global.values,\n                y_pred_probas_df[pair[0]].values,\n                y_pred_probas_df[pair[1]].values\n            )\n            res['p_value (DeLong)'] = p_delong\n\n            sub_df = pd.DataFrame([res])\n\n            pvals_bs = sub_df['p_value'].dropna()\n            if not pvals_bs.empty:\n                _, pvals_fdr, _, _ = multipletests(pvals_bs, alpha=0.05, method='fdr_bh')\n                sub_df['p-value (Bootstrap, FDR)'] = pvals_fdr\n\n            pvals_delong = sub_df['p_value (DeLong)'].dropna()\n            if not pvals_delong.empty:\n                _, pvals_fdr, _, _ = multipletests(pvals_delong, alpha=0.05, method='fdr_bh')\n                sub_df['p-value (DeLong, FDR)'] = pvals_fdr\n\n            table4_subtables[prefix] = sub_df\n\n    print(\"\\n\" + \"=\"*70)\n    print(\"--- Table 4: AUC Difference (Test Data) ---\")\n    print(\"=\"*70)\n\n    for prefix, df in table4_subtables.items():\n        print(f\"\\n--- {prefix} Model Comparisons ---\")\n        print(df.to_string(index=False))\n\n    save_tables_to_word(\n        table1_df, supp_table1_df, table2_df, table3_df, table4_subtables,\n        OUTPUT_DIR / f'Tables_{TARGET_COLUMN_FOR_LABELING}_Phase1.docx',\n        TARGET_VARIABLE\n    )\n\n    plot_roc_curves(\n        final_results, table2_df,\n        OUTPUT_DIR / f'ROC_Curve_{TARGET_COLUMN_FOR_LABELING}_Phase1.svg'\n    )\n\n    print(\"\\n\" + \"=\"*80)\n    print(\"### PHASE 1 ANALYSIS COMPLETE ###\")\n    print(\"=\"*80)\n\n    return TARGET_VARIABLE, df_orig, train_indices, test_indices"],
      "metadata": {"id": "phase1"},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": ["# ==============================================================================\n# Phase 2: Acoustic Only vs Acoustic + Clinical\n# ==============================================================================\ndef main_phase2():\n    print(\"\\n\" + \"=\"*80)\n    print(\"### STARTING PHASE 2 ANALYSIS ###\")\n    print(\"### Acoustic Only vs Acoustic + Clinical Features ###\")\n    print(\"=\"*80)\n\n    OUTPUT_DIR_P2 = DRIVE_BASE_PATH / f\"{TARGET_COLUMN_FOR_LABELING}_{GROUP_THRESHOLD}_results_phase2\"\n    OUTPUT_DIR_P2.mkdir(exist_ok=True)\n    print(f\"Phase 2 results will be saved to '{OUTPUT_DIR_P2}'\")\n\n    TARGET_VARIABLE, df_orig = load_and_preprocess_data(\n        INPUT_EXCEL_PATH, TARGET_COLUMN_FOR_LABELING, GROUP_THRESHOLD\n    )\n\n    print(\"\\n--- Phase 2: Preprocessing clinical and acoustic features ---\")\n    df_processed = df_orig.copy()\n\n    binary_cols = ['men', 'arterial calcification', 'DM', 'HTN', 'Heart disease', 'tabaco site', 'left']\n    mapping_dict = {'y': 1, 'n': 0, 'male': 1, 'female': 0, '1': 1, '0': 0, '1.0': 1, '0.0': 0}\n\n    for col in binary_cols:\n        if col in df_processed.columns:\n            df_processed[col] = df_processed[col].astype(str).str.lower().map(mapping_dict)\n            if df_processed[col].isnull().any():\n                df_processed[col].fillna(df_processed[col].mode()[0], inplace=True)\n\n    numeric_clinical_cols = ['age', 'POD', 'RA diameter', 'shunt diameter']\n    for col in numeric_clinical_cols:\n        if col in df_processed.columns:\n            df_processed[col] = pd.to_numeric(df_processed[col], errors='coerce')\n            if df_processed[col].isnull().any():\n                df_processed[col].fillna(df_processed[col].mean(), inplace=True)\n\n    categorical_cols = ['HD(a, b, c)']\n    valid_categorical = [c for c in categorical_cols if c in df_processed.columns]\n    if valid_categorical:\n        df_processed = pd.get_dummies(df_processed, columns=valid_categorical, drop_first=True, dtype=float)\n\n    acoustic_cols = [col for col in df_processed.columns if re.match(r\"^E\\d+\", col)]\n    for col in acoustic_cols:\n        df_processed[col] = pd.to_numeric(df_processed[col], errors='coerce').fillna(0)\n\n    y = df_processed[TARGET_VARIABLE]\n    groups = df_processed['Pt No']\n\n    train_indices, test_indices = split_data_by_patient(\n        df_processed, TARGET_VARIABLE, test_size=0.2, random_state=UNIFIED_RANDOM_STATE\n    )\n\n    print_patient_ids_summary(df_processed, train_indices, test_indices, TARGET_VARIABLE, OUTPUT_DIR_P2)\n\n    print(\"\\n--- Phase 2: Feature Selection ---\")\n    df_train = df_processed.loc[train_indices]\n    y_train = y.loc[train_indices]\n    groups_train = groups.loc[train_indices]\n\n    e_features = [col for col in df_processed.columns if re.match(r\"^E\\d+\", col)]\n    base_clinical = ['age', 'POD', 'men', 'arterial calcification', 'DM', 'HTN',\n                    'Heart disease', 'RA diameter', 'shunt diameter', 'tabaco site', 'left']\n    ohe_cols = [col for col in df_processed.columns if any(cat in col for cat in valid_categorical)] if valid_categorical else []\n    clinical_cols = [c for c in base_clinical + ohe_cols if c in df_processed.columns]\n\n    combined_pool = e_features + clinical_cols\n\n    feature_selector = RandomForestClassifier(\n        random_state=UNIFIED_RANDOM_STATE, class_weight='balanced'\n    )\n\n    print(\"\\nSelecting features for [E_only] model...\")\n    e_only_cols = select_stable_features(\n        df_train, y_train, groups_train, e_features,\n        feature_selector, N_FEATURES_TO_SELECT,\n        N_BOOTSTRAPS_STABILITY, UNIFIED_RANDOM_STATE\n    )\n\n    print(\"\\nSelecting features for [Acoustic_Plus_Clinical] model...\")\n    combined_cols = select_stable_features(\n        df_train, y_train, groups_train, combined_pool,\n        feature_selector, N_FEATURES_TO_SELECT,\n        N_BOOTSTRAPS_STABILITY, UNIFIED_RANDOM_STATE\n    )\n\n    feature_sets = {\n        \"E_only\": e_only_cols,\n        \"Acoustic_Plus_Clinical\": combined_cols\n    }\n\n    print(f\"\\nE_only features (n={len(e_only_cols)}): {e_only_cols}\")\n    print(f\"Acoustic_Plus_Clinical features (n={len(combined_cols)}): {combined_cols}\")\n\n    preprocessor = ColumnTransformer(\n        transformers=[\n            ('num', StandardScaler(), make_column_selector(dtype_include=np.number))\n        ],\n        remainder='passthrough'\n    )\n\n    models = {\n        \"LR\": Pipeline([\n            ('preprocessor', preprocessor),\n            ('model', LogisticRegression(\n                random_state=UNIFIED_RANDOM_STATE,\n                class_weight='balanced',\n                max_iter=1000\n            ))\n        ]),\n        \"RF\": RandomForestClassifier(\n            random_state=UNIFIED_RANDOM_STATE,\n            class_weight='balanced'\n        )\n    }\n\n    y_test_global = y.loc[test_indices]\n    test_patient_ids_global = groups.loc[test_indices]\n\n    final_results_p2 = {}\n    summary_data_p2 = []\n    y_pred_probas_test_p2 = {}\n\n    print(\"\\n--- Phase 2: Model Evaluation ---\")\n    for model_key, model_pipeline in models.items():\n        for feature_name, feature_list in feature_sets.items():\n            model_id = f\"{model_key}_{feature_name}\"\n            print(f\"\\nProcessing: {model_id}\")\n\n            X_selected = df_processed[feature_list]\n\n            cv_perf = evaluate_model_with_cv(\n                X_selected.loc[train_indices], y_train, groups_train, model_pipeline\n            )\n\n            X_train = X_selected.loc[train_indices]\n            X_test = X_selected.loc[test_indices]\n            y_train_loop = y.loc[train_indices]\n            y_test_loop = y.loc[test_indices]\n            groups_test = groups.loc[test_indices]\n\n            final_model = clone(model_pipeline).fit(X_train, y_train_loop)\n            proba_test = final_model.predict_proba(X_test)[:, 1]\n            y_pred_probas_test_p2[model_id] = pd.Series(proba_test, index=X_test.index)\n\n            ci_lower, ci_upper = calculate_bootstrap_ci(y_test_loop, proba_test, groups_test)\n\n            summary_data_p2.append({\n                'Model': model_id,\n                'Features': len(feature_list),\n                'AUC': f\"{cv_perf.loc['auc', 'mean']:.3f} ± {cv_perf.loc['auc', 'std']:.3f}\",\n                'AUC (95% CI)': f\"{ci_lower:.3f} - {ci_upper:.3f}\",\n                'Accuracy': f\"{cv_perf.loc['accuracy', 'mean']:.3f} ± {cv_perf.loc['accuracy', 'std']:.3f}\",\n                'Sensitivity': f\"{cv_perf.loc['sensitivity', 'mean']:.3f} ± {cv_perf.loc['sensitivity', 'std']:.3f}\",\n                'Specificity': f\"{cv_perf.loc['specificity', 'mean']:.3f} ± {cv_perf.loc['specificity', 'std']:.3f}\"\n            })\n\n            final_results_p2[model_id] = {\n                'model': final_model,\n                'X_test': X_test,\n                'y_test': y_test_loop,\n                'X_train': X_train\n            }\n\n    table_p2_df = pd.DataFrame(summary_data_p2)\n    print(\"\\n\" + \"=\"*50)\n    print(\"--- Phase 2: Model Performance Summary ---\")\n    print(\"=\"*50)\n    print(table_p2_df.to_string(index=False))\n\n    display_test_auc_summary(\n        final_results_p2, y_pred_probas_test_p2,\n        y_test_global, test_patient_ids_global, \"Phase 2\"\n    )\n\n    y_pred_probas_df_p2 = pd.DataFrame(y_pred_probas_test_p2)\n    table_diff_p2 = {}\n\n    for prefix in models.keys():\n        pair = (f'{prefix}_Acoustic_Plus_Clinical', f'{prefix}_E_only')\n\n        if all(p in y_pred_probas_df_p2.columns for p in pair):\n            res = compute_auc_diff_paired(y_test_global, y_pred_probas_df_p2, test_patient_ids_global, pair)\n            p_delong = delong_roc_test(\n                y_test_global.values,\n                y_pred_probas_df_p2[pair[0]].values,\n                y_pred_probas_df_p2[pair[1]].values\n            )\n            res['p_value (DeLong)'] = p_delong\n            table_diff_p2[prefix] = pd.DataFrame([res])\n\n    print(\"\\n\" + \"=\"*70)\n    print(\"--- Phase 2: AUC Difference (Acoustic vs. Integrated) ---\")\n    print(\"=\"*70)\n\n    for prefix, df in table_diff_p2.items():\n        print(f\"\\n--- {prefix} Model ---\")\n        print(df.to_string(index=False))\n\n    if not table_p2_df.empty:\n        model_id_best = \"LR_Acoustic_Plus_Clinical\"\n\n        if model_id_best not in final_results_p2:\n            cv_aucs = table_p2_df['AUC'].str.split(' ±', expand=True)[0].astype(float)\n            best_idx = cv_aucs.idxmax()\n            model_id_best = table_p2_df.loc[best_idx, 'Model']\n\n        best_results = final_results_p2[model_id_best]\n        print(f\"\\nDetailed analysis: '{model_id_best}'\")\n\n        analyze_and_plot_shap(\n            best_results['model'], best_results['X_train'], best_results['X_test'],\n            OUTPUT_DIR_P2, model_id_best, f\"{TARGET_COLUMN_FOR_LABELING}_Phase2\"\n        )\n\n        plot_calibration_curve(\n            best_results['model'], best_results['X_test'], y_test_global,\n            model_id_best, OUTPUT_DIR_P2, f\"{TARGET_COLUMN_FOR_LABELING}_Phase2\"\n        )\n\n    plot_roc_curves(\n        final_results_p2, table_p2_df,\n        OUTPUT_DIR_P2 / f'ROC_Curve_{TARGET_COLUMN_FOR_LABELING}_Phase2.svg'\n    )\n\n    print(\"\\n\" + \"=\"*80)\n    print(\"### PHASE 2 ANALYSIS COMPLETE ###\")\n    print(\"=\"*80)"],
      "metadata": {"id": "phase2"},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": ["# ==============================================================================\n# Main Execution\n# ==============================================================================\nif __name__ == \"__main__\":\n    overall_start_time = time.time()\n\n    main_phase1()\n    main_phase2()\n\n    overall_end_time = time.time()\n    print(f\"\\nTotal analysis time: {(overall_end_time - overall_start_time) / 60:.2f} minutes\")"],
      "metadata": {"id": "main"},
      "execution_count": null,
      "outputs": []
    }
  ]
}
